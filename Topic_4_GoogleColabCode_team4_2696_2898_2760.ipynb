{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeonardFreris/fleonardos_ece_uth_projects/blob/main/Topic_4_GoogleColabCode_team4_2696_2898_2760.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import python libraries"
      ],
      "metadata": {
        "id": "HalTwUXhIlcL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "! pip install pytorch-lightning\n",
        "! pip install pytorch-lightning-bolts"
      ],
      "metadata": {
        "id": "JRJGjI5aPb4N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xVtVfZSbCo2z"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy\n",
        "import array\n",
        "from sklearn import preprocessing\n",
        "import scipy.stats as stats\n",
        "\n",
        "from numpy import double\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data\n",
        "import pytorch_lightning as pl\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
        "torch.backends.cudnn.determinstic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Fetching the device that will be used throughout this notebook\n",
        "device = torch.device(\"cpu\") if not torch.cuda.is_available() else torch.device(\"cuda:0\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read dataset"
      ],
      "metadata": {
        "id": "KdeMvMOdIzTN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g9IQ2WldCxHD"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/compressed_dataset.csv\") # set df as the compressed_dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Rk0vetRf5Dp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Chpp2Mg5_AIa"
      },
      "source": [
        "# Engagement distribution over time, taking into account customer_id/country_id/city_id\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Barplot distributin of engagemnt over time taking into account customer_id"
      ],
      "metadata": {
        "id": "3vCxynAgfK0l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s1H5T2jB-43C"
      },
      "outputs": [],
      "source": [
        "sns.set(rc={'figure.figsize':(17.7,8.27)})\n",
        "ax = sns.barplot(x = \"customer_id\", y = \"engagement\", data = df, ci = 'sd', capsize = 0.3, errwidth = 2).set(title='Engagement distribution barplot taking into account customer_id')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2kjG-vp_lUi"
      },
      "source": [
        "From the task \"Countries/Cities that follow different distribution per customer\"\n",
        "we take two countries_id from every list of same distribution countries and picturize the following engagement barplots which represent every county_id of the same list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XwsAKgkb--dw"
      },
      "outputs": [],
      "source": [
        "countrysampleDF_0 = df.loc[df['country_id'] == 0]\n",
        "countrysampleDF_22 = df.loc[df['country_id'] == 22]\n",
        "\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (20,8))\n",
        "fig.suptitle('Histograms of engagement distribution for countries 0.0 and 22.0')\n",
        "sns.histplot(data=countrysampleDF_0, x=\"engagement\", ax=ax1)\n",
        "sns.histplot(data=countrysampleDF_22, x=\"engagement\", ax=ax2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "countrysampleDF_1 = df.loc[df['country_id'] == 1]\n",
        "countrysampleDF_20 = df.loc[df['country_id'] == 20]\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (20,8))\n",
        "fig.suptitle('Histograms of engagement distribution for countries 1.0 and 20.0')\n",
        "sns.histplot(data=countrysampleDF_1, x=\"engagement\", ax=ax1)\n",
        "sns.histplot(data=countrysampleDF_20, x=\"engagement\", ax=ax2)"
      ],
      "metadata": {
        "id": "1h8HemkqEl31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "countrysampleDF_60 = df.loc[df['country_id'] == 60]\n",
        "countrysampleDF_58 = df.loc[df['country_id'] == 58]\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (20,8))\n",
        "fig.suptitle('Histograms of engagement distribution for countries 60.0 and 58.0')\n",
        "#fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (20,8))\n",
        "sns.histplot(data=countrysampleDF_60, x=\"engagement\", ax=ax1)\n",
        "sns.histplot(data=countrysampleDF_58, x=\"engagement\", ax=ax2)"
      ],
      "metadata": {
        "id": "Q68jCuigFo65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hBnLsdB_-JB"
      },
      "source": [
        "From the task \"Countries/Cities that follow different distribution per customer\"\n",
        "we take random cities_id samples from every list of same distribution cities and picturize the following engagement barplots which represent every city_id of the same list"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "citysampleDF_0 = df.loc[df['city_id'] == 0]\n",
        "citysampleDF_30 = df.loc[df['city_id'] == 30]\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (20,8))\n",
        "fig.suptitle('Histograms of engagement distribution for cities 0.0 and 30.0')\n",
        "sns.histplot(data=citysampleDF_0, x=\"engagement\", ax=ax1)\n",
        "sns.histplot(data=citysampleDF_30, x=\"engagement\", ax=ax2)"
      ],
      "metadata": {
        "id": "4xSp0B6wG825"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "citysampleDF_1 = df.loc[df['city_id'] == 1]\n",
        "citysampleDF_133 = df.loc[df['city_id'] == 133]\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (20,8))\n",
        "fig.suptitle('Histograms of engagement distribution for countries 1.0 and 133.0')\n",
        "sns.histplot(data=citysampleDF_1, x=\"engagement\", ax=ax1)\n",
        "sns.histplot(data=citysampleDF_133, x=\"engagement\", ax=ax2)"
      ],
      "metadata": {
        "id": "xS5BC_KjNZ32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "citysampleDF_50 = df.loc[df['city_id'] == 50]\n",
        "citysampleDF_303 = df.loc[df['city_id'] == 303]\n",
        "\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (20,8))\n",
        "fig.suptitle('Histograms of engagement distribution for countries 50.0 and 303.0')\n",
        "sns.histplot(data=citysampleDF_50, x=\"engagement\", ax=ax1)\n",
        "sns.histplot(data=citysampleDF_303, x=\"engagement\", ax=ax2)"
      ],
      "metadata": {
        "id": "gipotFm4NyV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "citysampleDF_268 = df.loc[df['city_id'] == 268]\n",
        "sns.set(rc={'figure.figsize':(17.7,8.27)})\n",
        "ax = sns.histplot(data=citysampleDF_268, x=\"engagement\")"
      ],
      "metadata": {
        "id": "We9_rADqOET2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# QoE distribution over time taking into account customer_id/country_id/city_id"
      ],
      "metadata": {
        "id": "T4GNQTDxJVqi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Same process with engagment task but instead of engagement information we use Qoe"
      ],
      "metadata": {
        "id": "2RzqeMWvJbVF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set(rc={'figure.figsize':(17.7,8.27)})\n",
        "ax = sns.barplot(x=\"customer_id\", y=\"qoe\", data=df, ci='sd', capsize=0.3, errwidth=2).set(title='Qoe distribution barplot taking into account customer_id')"
      ],
      "metadata": {
        "id": "dO4B94IfJ4_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sample countries qoe distribution histograms are presented below"
      ],
      "metadata": {
        "id": "DAKjPEiXlanW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (20,8))\n",
        "fig.suptitle('Histograms of quality of experience distribution for countries 0.0 and 22.0')\n",
        "sns.histplot(data=countrysampleDF_0, x=\"qoe\", ax=ax1)\n",
        "sns.histplot(data=countrysampleDF_22, x=\"qoe\", ax=ax2)"
      ],
      "metadata": {
        "id": "nzdXdxQdKJ04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (20,8))\n",
        "fig.suptitle('Histograms of quality of experience distribution for countries 1.0 and 20.0')\n",
        "sns.histplot(data=countrysampleDF_1, x=\"qoe\", ax=ax1)\n",
        "sns.histplot(data=countrysampleDF_20, x=\"qoe\", ax=ax2)"
      ],
      "metadata": {
        "id": "bprg2jn8OyNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (20,8))\n",
        "fig.suptitle('Histograms of quality of experience distribution for countries 60.0 and 58.0')\n",
        "sns.histplot(data=countrysampleDF_60, x=\"qoe\", ax=ax1)\n",
        "sns.histplot(data=countrysampleDF_58, x=\"qoe\", ax=ax2)"
      ],
      "metadata": {
        "id": "3ny2dZJLlZeu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "for cities qoe distribution we create the following histograms"
      ],
      "metadata": {
        "id": "H5jPleqjji-c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (20,8))\n",
        "fig.suptitle('Histograms of quality of experience distribution for cities 0.0 and 30.0')\n",
        "sns.histplot(data=citysampleDF_0, x=\"qoe\", ax=ax1)\n",
        "sns.histplot(data=citysampleDF_30, x=\"qoe\", ax=ax2)"
      ],
      "metadata": {
        "id": "qEyBpGhkKv0F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (20,8))\n",
        "fig.suptitle('Histograms of quality of experience distribution for cities 1.0 and 133.0')\n",
        "sns.histplot(data=citysampleDF_1, x=\"qoe\", ax=ax1)\n",
        "sns.histplot(data=citysampleDF_133, x=\"qoe\", ax=ax2)"
      ],
      "metadata": {
        "id": "bS4j1K8SQGRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (20,8))\n",
        "fig.suptitle('Histograms of quality of experience distribution for cities 50.0 and 303.0')\n",
        "sns.histplot(data=citysampleDF_50, x=\"qoe\", ax=ax1)\n",
        "sns.histplot(data=citysampleDF_303, x=\"qoe\", ax=ax2)"
      ],
      "metadata": {
        "id": "JeVgDTwnjqmQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sns.histplot(data=citysampleDF_268, x=\"qoe\")"
      ],
      "metadata": {
        "id": "g9o-kJOkQW0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hg2v7IhKAA5G"
      },
      "source": [
        "# Engagement/Qoe distribution differences based on viewer_type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HNB_cPjnAd1l"
      },
      "outputs": [],
      "source": [
        "sns.set(rc={'figure.figsize':(8.7,8.27)})\n",
        "ax = sns.barplot(x=\"viewer_type\", y=\"engagement\", data=df, ci=\"sd\").set(title='Engagement distribution barplot taking into account viewer_type')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NqARXC__A1Gq"
      },
      "outputs": [],
      "source": [
        "sns.set(rc={'figure.figsize':(8.7,8.27)})\n",
        "ax = sns.barplot(x=\"viewer_type\", y=\"qoe\", data=df, ci=\"sd\").set(title='Qoe distribution barplot taking into account viewer_type')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Viewers’ engagement level duration over country/city/viewer type"
      ],
      "metadata": {
        "id": "VrYUfMhZTAH0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Proccess to find viewers’ engagement level duration over country_id:***"
      ],
      "metadata": {
        "id": "OAbnov2yC7Kf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We keep the columns to help us find the viewers’ engagement level duration over country"
      ],
      "metadata": {
        "id": "hVgLwYA5wzWs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_m = df.loc[:,['timestamp','viewer_id','country_id', 'engagement']]\n",
        "df_m"
      ],
      "metadata": {
        "id": "YqpLMM1-TEmg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we sort the minimized dataframe first by country_id then by viewer_id and finally by timestamp"
      ],
      "metadata": {
        "id": "PCfQyOfpw5Px"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_s = df_m.sort_values(['country_id', 'viewer_id', 'timestamp'], ascending=[True, True, True])\n",
        "df_s.tail(50)"
      ],
      "metadata": {
        "id": "nhUvLSbyZHex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We convert the sorted dataframe to array and we store them to three vectors of each level duration"
      ],
      "metadata": {
        "id": "5NmDqilvxAAR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "array = df_s.to_numpy()\n",
        "array"
      ],
      "metadata": {
        "id": "QHbwZ6XwAGX9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute level(bad, medium, good) duration of engagement over country"
      ],
      "metadata": {
        "id": "GWl8yD9pxE4-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "j = 0\n",
        "badEng = 0\n",
        "medEng = 0\n",
        "goodEng = 0\n",
        "goodEng_vec = []\n",
        "medEng_vec = []\n",
        "badEng_vec = []\n",
        "\n",
        "viewers = df_s['viewer_id'].unique()\n",
        "p = df_s.shape[0]\n",
        "for j in range(p):\n",
        "  if (j==p-1):\n",
        "    goodEng_vec.append(goodEng)\n",
        "    medEng_vec.append(medEng)\n",
        "    badEng_vec.append(badEng)\n",
        "    print(\"good eng\", goodEng_vec)\n",
        "    print(\"med eng\", medEng_vec)\n",
        "    print(\"bad eng\", badEng_vec)\n",
        "    break\n",
        "  if(array[j][2] == array[j+1][2]): #check next country\n",
        "    if(array[j][1] == array[j+1][1]): #check next viewer_id\n",
        "      timeDiff = array[j][0] - array[j-1][0]\n",
        "      if(((timeDiff) >= 29000) & ((timeDiff) <= 31000)):\n",
        "        if(((array[j+1][3] <= 0.3) & (array[j+1][3] >= 0)) & ((array[j][3] <= 0.3) & (array[j][3] >= 0))):\n",
        "          badEng += timeDiff\n",
        "        elif (((array[j+1][3] <= 0.7) & (array[j+1][3] > 0.3)) & ((array[j][3] <= 0.7) & (array[j][3] > 0.3))):\n",
        "          medEng += timeDiff\n",
        "        elif(((array[j+1][3] <= 1) & (array[j+1][3] > 0.7)) & ((array[j][3] <= 1) & (array[j][3] > 0.7))):\n",
        "          goodEng += timeDiff\n",
        "  else: \n",
        "    goodEng_vec.append(goodEng)\n",
        "    medEng_vec.append(medEng)\n",
        "    badEng_vec.append(badEng) \n",
        "    badEng=0\n",
        "    medEng=0\n",
        "    goodEng=0"
      ],
      "metadata": {
        "id": "qNS37Izhu_K8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find the countries with zero duration on good, medium and bad level engagement and the top 5 countries with the most duration"
      ],
      "metadata": {
        "id": "rO1-z33ixOk-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "goodEng_sort = np.sort(goodEng_vec)\n",
        "i=0\n",
        "k=len(goodEng_vec)\n",
        "goodEng_zero = []\n",
        "goodEng_top = []\n",
        "\n",
        "for i in range(k):\n",
        "  if(0==goodEng_vec[i]):\n",
        "    goodEng_zero.append(i)\n",
        "  elif((goodEng_sort[k-1]==goodEng_vec[i]) | (goodEng_sort[k-2]==goodEng_vec[i]) | (goodEng_sort[k-3]==goodEng_vec[i]) | (goodEng_sort[k-4]==goodEng_vec[i]) | (goodEng_sort[k-5]==goodEng_vec[i])):\n",
        "    goodEng_top.append(i)\n",
        "  \n",
        "print(\"Countries with 0 duration of good Engagement: \",goodEng_zero)\n",
        "print(\"Top 5 countries with max duration of good Engagement: \", goodEng_top)"
      ],
      "metadata": {
        "id": "Uo5uEjyIvNjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "medEng_sort = np.sort(medEng_vec)\n",
        "i=0\n",
        "k=len(medEng_vec)\n",
        "medEng_zero = []\n",
        "medEng_top = []\n",
        "\n",
        "for i in range(k):\n",
        "  if(0==medEng_vec[i]):\n",
        "    medEng_zero.append(i)\n",
        "  elif((medEng_sort[k-1]==medEng_vec[i]) | (medEng_sort[k-2]==medEng_vec[i]) | (medEng_sort[k-3]==medEng_vec[i]) | (medEng_sort[k-4]==medEng_vec[i]) | (medEng_sort[k-5]==medEng_vec[i])):\n",
        "    medEng_top.append(i)\n",
        "  \n",
        "print(\"Countries with 0 duration of medium Engagement: \",medEng_zero)\n",
        "print(\"Top 5 countries with max duration of medium Engagement: \", medEng_top)"
      ],
      "metadata": {
        "id": "10sejL9dvTNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "badEng_sort = np.sort(badEng_vec)\n",
        "i=0\n",
        "k=len(badEng_vec)\n",
        "badEng_zero = []\n",
        "badEng_top = []\n",
        "\n",
        "for i in range(k):\n",
        "  if(0==badEng_vec[i]):\n",
        "    badEng_zero.append(i)\n",
        "  elif((badEng_sort[k-1]==badEng_vec[i]) | (badEng_sort[k-2]==badEng_vec[i]) | (badEng_sort[k-3]==badEng_vec[i]) | (badEng_sort[k-4]==badEng_vec[i]) | (badEng_sort[k-5]==badEng_vec[i])):\n",
        "    badEng_top.append(i)\n",
        "  \n",
        "print(\"Countries with 0 duration of medium Engagement: \",badEng_zero)\n",
        "print(\"Top 5 countries with max duration of medium Engagement: \", badEng_top)"
      ],
      "metadata": {
        "id": "uubPVd-RvWw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We plot side by side the top 5 countries with maximum duration for each engegament level"
      ],
      "metadata": {
        "id": "LyXv2ijOxWqk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "i=0\n",
        "k=len(badEng_vec)\n",
        "country=[]\n",
        "for i in range(k):\n",
        "  country.append(i)\n",
        "\n",
        "data_dur=pd.DataFrame({'goodEng_dur':goodEng_vec, 'medEng_dur':medEng_vec, 'badEng_dur':badEng_vec, 'country':country})\n",
        "data_dur"
      ],
      "metadata": {
        "id": "CFlRCZjJvcHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize = (20,8))\n",
        "fig.suptitle('Top 5 countries with maximum duration for eache engegament level')\n",
        "top5_good = data_dur[data_dur[\"country\"] < 5]\n",
        "sns.barplot(x=\"country\", y='goodEng_dur', data = top5_good, ax=ax1)\n",
        "\n",
        "top5_med = data_dur[data_dur[\"country\"] < 5]\n",
        "sns.barplot(x=\"country\", y='medEng_dur', data = top5_med, ax=ax2)\n",
        "\n",
        "top5_bad = data_dur[(data_dur[\"country\"]<4)]\n",
        "top5_bad = top5_bad.append(data_dur[(data_dur[\"country\"]==5)])\n",
        "sns.barplot(x=\"country\", y='badEng_dur', data = top5_bad, ax=ax3)"
      ],
      "metadata": {
        "id": "JpMZ9pZEvfXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Proccess to find viewers’ engagement level duration over city_id:***"
      ],
      "metadata": {
        "id": "nk_Hj9zFCugN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Same proccess to find viewers’ engagement level duration over city as we did on country_id"
      ],
      "metadata": {
        "id": "IRoPEVDvxeR6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_city = df.loc[:,['timestamp','viewer_id','city_id', 'engagement']]\n",
        "df_citySort = df_city.sort_values(['city_id', 'viewer_id', 'timestamp'], ascending=[True, True, True])\n",
        "array_city = df_citySort.to_numpy()"
      ],
      "metadata": {
        "id": "pu2I1-6aviFd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "j = 0\n",
        "badEng_city = 0\n",
        "medEng_city = 0\n",
        "goodEng_city = 0\n",
        "goodEng_vec_city = []\n",
        "medEng_vec_city = []\n",
        "badEng_vec_city = []\n",
        "\n",
        "viewers = df_citySort['viewer_id'].unique()\n",
        "p = df_citySort.shape[0]\n",
        "for j in range(p):\n",
        "  if (j==p-1):\n",
        "    goodEng_vec_city.append(goodEng_city)\n",
        "    medEng_vec_city.append(medEng_city)\n",
        "    badEng_vec_city.append(badEng_city) \n",
        "    print(\"good eng over city\", goodEng_vec_city)\n",
        "    print(\"med eng over city\", medEng_vec_city)\n",
        "    print(\"bad eng over city\", badEng_vec_city)\n",
        "    break\n",
        "  if(array_city[j][2] == array_city[j+1][2]): #check next city\n",
        "    if(array_city[j][1] == array_city[j+1][1]): #check next viewer_id\n",
        "      timeDiff_city = array_city[j][0] - array_city[j-1][0]\n",
        "      if(((timeDiff_city) >= 29000) & ((timeDiff_city) <= 31000)):\n",
        "        if(((array_city[j+1][3] <= 0.3) & (array_city[j+1][3] >= 0)) & ((array_city[j][3] <= 0.3) & (array_city[j][3] >= 0))):\n",
        "          badEng_city += timeDiff_city\n",
        "        elif (((array_city[j+1][3] <= 0.7) & (array_city[j+1][3] > 0.3)) & ((array_city[j][3] <= 0.7) & (array_city[j][3] > 0.3))):\n",
        "          medEng_city += timeDiff_city\n",
        "        elif(((array_city[j+1][3] <= 1) & (array_city[j+1][3] > 0.7)) & ((array_city[j][3] <= 1) & (array_city[j][3] > 0.7))):\n",
        "          goodEng_city += timeDiff_city\n",
        "  else: \n",
        "    goodEng_vec_city.append(goodEng_city)\n",
        "    medEng_vec_city.append(medEng_city)\n",
        "    badEng_vec_city.append(badEng_city)\n",
        "    badEng_city=0\n",
        "    medEng_city=0\n",
        "    goodEng_city=0"
      ],
      "metadata": {
        "id": "URHhla3EvypO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "badEng_sort_city = np.sort(badEng_vec_city)\n",
        "goodEng_sort_city = np.sort(goodEng_vec_city)\n",
        "i=0\n",
        "k=len(goodEng_vec_city)\n",
        "goodEng_zero_city = []\n",
        "goodEng_top_city = []\n",
        "\n",
        "for i in range(k):\n",
        "  if(0==goodEng_vec_city[i]):\n",
        "    goodEng_zero_city.append(i)\n",
        "  elif((goodEng_sort_city[k-1]==goodEng_vec_city[i]) | (goodEng_sort_city[k-2]==goodEng_vec_city[i]) | (goodEng_sort_city[k-3]==goodEng_vec_city[i]) | (goodEng_sort_city[k-4]==goodEng_vec_city[i]) | (goodEng_sort_city[k-5]==goodEng_vec_city[i]) | (goodEng_sort_city[k-6]==goodEng_vec_city[i]) | (goodEng_sort_city[k-7]==goodEng_vec_city[i]) | (goodEng_sort_city[k-8]==goodEng_vec_city[i]) | (goodEng_sort_city[k-9]==goodEng_vec_city[i]) | (goodEng_sort_city[k-10]==goodEng_vec_city[i])):\n",
        "    goodEng_top_city.append(i)\n",
        "  \n",
        "print(\"Cities with 0 duration of good Engagement: \",goodEng_zero_city)\n",
        "print(\"Top 10 cities with max duration of good Engagement: \", goodEng_top_city)"
      ],
      "metadata": {
        "id": "S7GrqyXnv1d1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "medEng_sort_city = np.sort(medEng_vec_city)\n",
        "i=0\n",
        "k=len(medEng_vec_city)\n",
        "medEng_zero_city = []\n",
        "medEng_top_city = []\n",
        "\n",
        "for i in range(k):\n",
        "  if(0==medEng_vec_city[i]):\n",
        "    medEng_zero_city.append(i)\n",
        "  elif((medEng_sort_city[k-1]==medEng_vec_city[i]) | (medEng_sort_city[k-2]==medEng_vec_city[i]) | (medEng_sort_city[k-3]==medEng_vec_city[i]) | (medEng_sort_city[k-4]==medEng_vec_city[i]) | (medEng_sort_city[k-5]==medEng_vec_city[i]) | (medEng_sort_city[k-6]==medEng_vec_city[i]) | (medEng_sort_city[k-7]==medEng_vec_city[i]) | (medEng_sort_city[k-8]==medEng_vec_city[i]) | (medEng_sort_city[k-9]==medEng_vec_city[i]) | (medEng_sort_city[k-10]==medEng_vec_city[i])):\n",
        "    medEng_top_city.append(i)\n",
        "  \n",
        "print(\"Cities with 0 duration of medium Engagement: \",medEng_zero_city)\n",
        "print(\"Top 5 cities with max duration of medium Engagement: \", medEng_top_city)"
      ],
      "metadata": {
        "id": "SoPNzWH6v38h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "badEng_sort_city = np.sort(badEng_vec_city)\n",
        "i=0\n",
        "k=len(badEng_vec_city)\n",
        "badEng_zero_city = []\n",
        "badEng_top_city = []\n",
        "\n",
        "for i in range(k):\n",
        "  if(0==badEng_vec_city[i]):\n",
        "    badEng_zero_city.append(i)\n",
        "  elif((badEng_sort_city[k-1]==badEng_vec_city[i]) | (badEng_sort_city[k-2]==badEng_vec_city[i]) | (badEng_sort_city[k-3]==badEng_vec_city[i]) | (badEng_sort_city[k-4]==badEng_vec_city[i]) | (badEng_sort_city[k-5]==badEng_vec_city[i]) | (badEng_sort_city[k-6]==badEng_vec_city[i]) | (badEng_sort_city[k-7]==badEng_vec_city[i]) | (badEng_sort_city[k-8]==badEng_vec_city[i]) | (badEng_sort_city[k-9]==badEng_vec_city[i]) | (badEng_sort_city[k-10]==badEng_vec_city[i])):\n",
        "    badEng_top_city.append(i)\n",
        "  \n",
        "print(\"Cities with 0 duration of medium Engagement: \",badEng_zero_city)\n",
        "print(\"Top 5 cities with max duration of medium Engagement: \", badEng_top_city)"
      ],
      "metadata": {
        "id": "REIMXXCKv7_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i=0\n",
        "k=len(badEng_vec_city)\n",
        "city=[]\n",
        "for i in range(k):\n",
        "  city.append(i)\n",
        "\n",
        "data_dur_city=pd.DataFrame({'goodEng_dur_city':goodEng_vec_city, 'medEng_dur_city':medEng_vec_city, 'badEng_dur_city':badEng_vec_city, 'city':city})\n",
        "data_dur_city"
      ],
      "metadata": {
        "id": "0qi7tnzgv_jK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize = (20,8))\n",
        "fig.suptitle('Top 5 cities with maximum duration for eache engegament level')\n",
        "\n",
        "top5_good_city = data_dur_city[data_dur_city[\"city\"] < 9]\n",
        "top5_good_city = top5_good_city.append(data_dur_city[(data_dur_city[\"city\"]==14)])\n",
        "sns.barplot(x=\"city\", y='goodEng_dur_city', data = top5_good_city, ax=ax1)\n",
        "\n",
        "top5_med_city = data_dur_city[data_dur_city[\"city\"] < 6]\n",
        "top5_med_city = top5_med_city.append(data_dur_city[(data_dur_city[\"city\"]==7)])\n",
        "top5_med_city = top5_med_city.append(data_dur_city[(data_dur_city[\"city\"]==10)])\n",
        "top5_med_city = top5_med_city.append(data_dur_city[(data_dur_city[\"city\"]==15)])\n",
        "top5_med_city = top5_med_city.append(data_dur_city[(data_dur_city[\"city\"]==42)])\n",
        "sns.barplot(x=\"city\", y='medEng_dur_city', data = top5_med_city, ax=ax2)\n",
        "\n",
        "top5_bad_city = data_dur_city[data_dur_city[\"city\"] < 7]\n",
        "top5_bad_city = top5_bad_city.append(data_dur_city[(data_dur_city[\"city\"]==9)])\n",
        "top5_bad_city = top5_bad_city.append(data_dur_city[(data_dur_city[\"city\"]==10)])\n",
        "top5_bad_city = top5_bad_city.append(data_dur_city[(data_dur_city[\"city\"]==11)])\n",
        "sns.barplot(x=\"city\", y='badEng_dur_city', data = top5_bad_city, ax=ax3)"
      ],
      "metadata": {
        "id": "O_r744QRwPoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Proccess to find viewers’ engagement level duration over viewer_type:***"
      ],
      "metadata": {
        "id": "o0FrdTAxxmxq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We keep the columns to help us find the viewers' engagement level duration over viewer_type, sort them first by viewer_type, then by viewer_id and finally by timestampt. In addition we convert this new dataframe into array in order to have more agility among the data"
      ],
      "metadata": {
        "id": "3VSpAZ_mxpYq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_vt = df.loc[:,['timestamp','viewer_id','viewer_type', 'engagement']]\n",
        "df_vtSort = df_vt.sort_values(['viewer_type', 'viewer_id', 'timestamp'], ascending=[True, True, True])\n",
        "array_vt = df_vtSort.to_numpy()"
      ],
      "metadata": {
        "id": "68tSznHswTAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Computation of viewers’ engagement level duration over WFH and WFO"
      ],
      "metadata": {
        "id": "JfNrNGRGxx5U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "j = 0\n",
        "badEng_vt = 0\n",
        "medEng_vt = 0\n",
        "goodEng_vt = 0\n",
        "goodEng_vec_vt = []\n",
        "medEng_vec_vt = []\n",
        "badEng_vec_vt = []\n",
        "\n",
        "viewers = df_vtSort['viewer_id'].unique()\n",
        "p = df_vtSort.shape[0]\n",
        "for j in range(p):\n",
        "  if (j==p-1):\n",
        "    goodEng_vec_vt.append(goodEng_vt)\n",
        "    medEng_vec_vt.append(medEng_vt)\n",
        "    badEng_vec_vt.append(badEng_vt) \n",
        "    print(\"good eng over WFH and WFO\", goodEng_vec_vt)\n",
        "    print(\"med eng over  WFH and WFO\", medEng_vec_vt)\n",
        "    print(\"bad eng over  WFH and WFO\", badEng_vec_vt)\n",
        "    break\n",
        "  if(array_vt[j][2] == array_vt[j+1][2]): #check next viewer_type\n",
        "    if(array_vt[j][1] == array_vt[j+1][1]): #check next viewer_id\n",
        "      timeDiff_vt = array_vt[j][0] - array_vt[j-1][0]\n",
        "      if(((timeDiff_vt) >= 29000) & ((timeDiff_vt) <= 31000)):\n",
        "        if(((array_vt[j+1][3] <= 0.3) & (array_vt[j+1][3] >= 0)) & ((array_vt[j][3] <= 0.3) & (array_vt[j][3] >= 0))):\n",
        "          badEng_vt += timeDiff_vt\n",
        "        elif (((array_vt[j+1][3] <= 0.7) & (array_vt[j+1][3] > 0.3)) & ((array_vt[j][3] <= 0.7) & (array_vt[j][3] > 0.3))):\n",
        "          medEng_vt += timeDiff_vt\n",
        "        elif(((array_vt[j+1][3] <= 1) & (array_vt[j+1][3] > 0.7)) & ((array_vt[j][3] <= 1) & (array_vt[j][3] > 0.7))):\n",
        "          goodEng_vt += timeDiff_vt\n",
        "  else: \n",
        "    goodEng_vec_vt.append(goodEng_vt)\n",
        "    medEng_vec_vt.append(medEng_vt)\n",
        "    badEng_vec_vt.append(badEng_vt)\n",
        "    badEng_vt=0\n",
        "    medEng_vt=0\n",
        "    goodEng_vt=0"
      ],
      "metadata": {
        "id": "xKPDjkTOwWgC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We plot side by side the viewer's engegament level duration working from home and working from office"
      ],
      "metadata": {
        "id": "P4fP2I0xx3-L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "i=0\n",
        "viewer_type = ['WFH' ,'WFO']\n",
        "data_dur_vt=pd.DataFrame({'goodEng_dur_vt':goodEng_vec_vt, 'medEng_dur_vt':medEng_vec_vt, 'badEng_dur_vt':badEng_vec_vt, 'viewer_type':viewer_type})\n",
        "data_dur_vt"
      ],
      "metadata": {
        "id": "U-Xuxc8owhWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize = (20,8))\n",
        "fig.suptitle('Viewers engegament level duration WFH & WHFO')\n",
        "\n",
        "sns.barplot(x=\"viewer_type\", y='goodEng_dur_vt', data = data_dur_vt, ax=ax1)\n",
        "sns.barplot(x=\"viewer_type\", y='medEng_dur_vt', data = data_dur_vt, ax=ax2)\n",
        "sns.barplot(x=\"viewer_type\", y='badEng_dur_vt', data = data_dur_vt, ax=ax3)"
      ],
      "metadata": {
        "id": "TPfKu0udwZbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Countries/Cities that follow different distribution per customer"
      ],
      "metadata": {
        "id": "P1d_DamfE_jz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We create an array called final that contains the appearance probability each customer for every country_id"
      ],
      "metadata": {
        "id": "ACtIyj5kGfI4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wIqC5ZTcwoRx"
      },
      "outputs": [],
      "source": [
        "i=0\n",
        "percentage_array = [0]*33\n",
        "k=df[\"country_id\"].value_counts().shape[0]\n",
        "for i in range(k):\n",
        "  subsetDf = df.loc[ lambda x : (x['country_id'] == i).tolist()]\n",
        "  temp = subsetDf['customer_id'].value_counts().reindex(\n",
        "    df.customer_id.unique(), fill_value = 0).sort_index()\n",
        "  percentage_data = temp/temp.sum()\n",
        "  percentage_vector =  percentage_data.to_numpy()\n",
        "  percentage_array = np.vstack([percentage_array, percentage_vector])\n",
        "final = np.delete(percentage_array, (0), axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We create a recursive function to place the similar distribution countries into different lists. The comparison fulfilled withe use of Kolmogorov–Smirnov test\n"
      ],
      "metadata": {
        "id": "dJBQPxoCIiy1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import ks_2samp\n",
        "def recurse(dists):\n",
        "  same = []\n",
        "  diff = []\n",
        "  if dists == []:\n",
        "    return\n",
        "  j = 0\n",
        "  for j in range(len(dists)):\n",
        "    (ks, p) = ks_2samp(final[dists[0]], final[dists[j]])\n",
        "    if  p < 0.01:\n",
        "      diff.append(dists[j])\n",
        "    else:\n",
        "      same.append(dists[j]) \n",
        "  print(same)\n",
        "  recurse(diff)\n",
        "i=0\n",
        "dists = []\n",
        "same = []\n",
        "dif = []\n",
        "for i in range(k):\n",
        "  dists.append(i)\n",
        "recurse(dists)"
      ],
      "metadata": {
        "id": "2G7neHLayDdl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We plot an indicative distribution for one country of every list we created above using KDE method"
      ],
      "metadata": {
        "id": "6kMNOmTTImZL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dist_0_1_15 = df[(df[\"country_id\"] == 13 ) ]\n",
        "dist_0_1_15 = dist_0_1_15.append(df[(df[\"country_id\"] == 20 ) ])\n",
        "dist_0_1_15 = dist_0_1_15.append(df[(df[\"country_id\"] == 55 ) ])\n",
        "sns.displot(\n",
        "    data = dist_0_1_15,\n",
        "    x = \"customer_id\",\n",
        "    hue=\"country_id\",\n",
        "    kind=\"kde\",\n",
        "    aspect=1.4,\n",
        "    height=6,\n",
        "    palette = sns.color_palette(\"hls\", 3)\n",
        ")"
      ],
      "metadata": {
        "id": "zeJEUxTmyUGX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Same process as for the countries distribution to place same distribution cities into lists in order to see the differnces"
      ],
      "metadata": {
        "id": "_BOv9_1NJZUn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "i=0\n",
        "percentage_array_city = [0]*33\n",
        "k=df[\"city_id\"].value_counts().shape[0]\n",
        "for i in range(k):\n",
        "  subsetDf = df.loc[ lambda x : (x['city_id'] == i).tolist()]\n",
        "  temp_city = subsetDf['customer_id'].value_counts().reindex(\n",
        "    df.customer_id.unique(), fill_value = 0).sort_index()\n",
        "  percentage_data_city = temp_city/temp_city.sum()\n",
        "  percentage_vector_city =  percentage_data_city.to_numpy()\n",
        "  percentage_array_city = np.vstack([percentage_array_city, percentage_vector_city])\n",
        "final_city = np.delete(percentage_array_city, (0), axis=0)"
      ],
      "metadata": {
        "id": "EjKMQuslJ3Nq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import ks_2samp\n",
        "def recurse(dists_city):\n",
        "  same_city = []\n",
        "  diff_city = []\n",
        "  if dists_city == []:\n",
        "    return\n",
        "  j = 0\n",
        "  for j in range(len(dists_city)):\n",
        "    (ks, p) = ks_2samp(final_city[dists_city[0]], final_city[dists_city[j]])\n",
        "    if  p < 0.1:\n",
        "      diff_city.append(dists_city[j])\n",
        "    else:\n",
        "      same_city.append(dists_city[j]) \n",
        "  print(same_city)\n",
        "  recurse(diff_city)\n",
        "i = 0\n",
        "dists_city = []\n",
        "same_city = []\n",
        "diff_city = []\n",
        "for i in range(k):\n",
        "  dists_city.append(i)\n",
        "recurse(dists_city)"
      ],
      "metadata": {
        "id": "PLTy2iXY519K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use one city_id from every list to pictrurize the differnces between the lists with the same distributions"
      ],
      "metadata": {
        "id": "FEc10amjLA8m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "distCities_44_139_50_268 = df[(df[\"city_id\"] == 44)]\n",
        "distCities_44_139_50_268 = distCities_44_139_50_268.append(df[(df[\"city_id\"] == 139)])\n",
        "distCities_44_139_50_268 = distCities_44_139_50_268.append(df[(df[\"city_id\"] == 50)])\n",
        "distCities_44_139_50_268 = distCities_44_139_50_268.append(df[(df[\"city_id\"] == 268)])\n",
        "sns.displot (\n",
        "    data = distCities_44_139_50_268,\n",
        "    x = \"customer_id\",\n",
        "    hue=\"city_id\",\n",
        "    kind=\"kde\",\n",
        "    aspect=1.4,\n",
        "    height=6,\n",
        "    palette = sns.color_palette(\"hls\", 4)\n",
        ")"
      ],
      "metadata": {
        "id": "kAmeFjFTXsT_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Correlations between the data points mentioned in Section 2 of the Topic 4"
      ],
      "metadata": {
        "id": "mfth4LQHcI07"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Heatmap of linear correlation via Pearson method between all datapoints of our dataframe"
      ],
      "metadata": {
        "id": "l1n0oF-3MCeD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "viewer_type_le = preprocessing.LabelEncoder()\n",
        "df[\"viewer_type_le\"] = viewer_type_le.fit_transform(df[\"viewer_type\"])"
      ],
      "metadata": {
        "id": "W99AS6cCG8BH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Keep the columns in the right form"
      ],
      "metadata": {
        "id": "HUVkS2VWYKdo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cols = df.loc[:,['buffer_ms', 'timestamp', 'viewer_id', 'event_id', 'customer_id', 'country_id', 'city_id', 'qoe', 'engagement', 'viewer_type_le']]\n",
        "cols"
      ],
      "metadata": {
        "id": "7wL8ZEzTYHGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set(rc = {'figure.figsize':(11.7,10.27)})\n",
        "sns.heatmap(cols.corr(method = 'pearson'), annot = True, cmap = \"coolwarm\").set(title='Linear correlation via Pearson')"
      ],
      "metadata": {
        "id": "VyrdkHc9cwtj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Heatmap of non-linear correlation via Spearman method between all the datapoints of our dataframe"
      ],
      "metadata": {
        "id": "wOhrC8CUMlnT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set(rc={'figure.figsize':(11.7,10.27)})\n",
        "sns.heatmap(cols.corr(method = 'spearman'), annot = True, cmap = \"coolwarm\").set(title='Non-Linear correlation via Spearman')"
      ],
      "metadata": {
        "id": "6PWokj8BhdM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Correlations between viewer engagement and the following factors:"
      ],
      "metadata": {
        "id": "F2_ritjYW_YS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**i) Number of viewers during the event**"
      ],
      "metadata": {
        "id": "-dVO_JnGXGNa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We isolate the columns event_id, engagement, viewer_id of our initial dataframe into a new one and then we group it by event_id.\n",
        "\n",
        "Then we calculate the number of viewers and the mean engagement in every event"
      ],
      "metadata": {
        "id": "jQDw5zVjNd_D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tempEV = df.groupby(['event_id'], as_index=False)['viewer_id'].count()\n",
        "tempEE = df.groupby(['event_id'], as_index=False)['engagement'].mean()\n",
        "groupedbyEvent = tempEE.join(tempEV['viewer_id'])\n",
        "groupedbyEvent.rename(columns = {'viewer_id':'number_of_viewers', 'engagement':'avg_engagement'}, inplace = True)\n",
        "groupedbyEvent"
      ],
      "metadata": {
        "id": "B4gWMPf2XMLe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear correlation between the avg_angamenet and the number_of_viewers of the events with Pearson method"
      ],
      "metadata": {
        "id": "wgBwGMANNx4z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "groupedbyEvent['avg_engagement'].corr(groupedbyEvent['number_of_viewers'], method = 'pearson') "
      ],
      "metadata": {
        "id": "v_HAO7cNZXCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Non-Linear correlation between the avg_angamenet and the number_of_viewers of the events with Spearman method"
      ],
      "metadata": {
        "id": "lyKbO0B5PSCK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "groupedbyEvent['avg_engagement'].corr(groupedbyEvent['number_of_viewers'], method = 'spearman') "
      ],
      "metadata": {
        "id": "b3ZmDmM1Z1MI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ii) Day of the event**"
      ],
      "metadata": {
        "id": "UC3cdu8PPzn0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We translate the timestamp into datetime in a new columna. Then knowing datime which day of the week it is."
      ],
      "metadata": {
        "id": "LfuvRolSiSQK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['datetime'] = pd.to_datetime(df['timestamp'], unit='ms')\n",
        "df['dayWeek'] = df['datetime'].dt.day_name()\n",
        "df.tail(1000)"
      ],
      "metadata": {
        "id": "6j83h071P0SC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We find the correlation ratio between the day of the week and the engagement creating a function which corellate a numerical and a categorical datapoints."
      ],
      "metadata": {
        "id": "lzKRfnC_insl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import f_oneway\n",
        "bydayGroupList = df.groupby('dayWeek')['engagement'].apply(list)\n",
        "AnovaResults = f_oneway(*bydayGroupList)\n",
        "print(f'{AnovaResults[1]:.100f}')\n",
        "AnovaResults[0].item()"
      ],
      "metadata": {
        "id": "hPr5B_4NUnBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**iii) Duration of the event**"
      ],
      "metadata": {
        "id": "OOe4sNspjUw5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a subset that includes the columns: event_id and timestamp. Then we sort the events from maximum to menimum id and for every event we sort it's timestamps. Finally we turn the sorted subset into a numpy array."
      ],
      "metadata": {
        "id": "PFXpbcEUkGv4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tempDE = df.loc[:,['event_id','timestamp']]\n",
        "tempDESorted = tempDE.sort_values(['event_id', 'timestamp'], ascending=[True, True])\n",
        "arrayDE = tempDESorted.to_numpy()"
      ],
      "metadata": {
        "id": "HtcbTOInj2Zh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We calculate the duration of each event"
      ],
      "metadata": {
        "id": "HgDTKZS5k4tf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "events = tempDESorted['event_id'].shape[0]\n",
        "i=0\n",
        "counter=0\n",
        "duration_vec = []\n",
        "for i in range(events):\n",
        "  if (i == events - 1):\n",
        "    duration = arrayDE[i][1] - arrayDE[i-counter][1]\n",
        "    duration_vec.append(duration)\n",
        "    break\n",
        "  if(arrayDE[i][0] == arrayDE[i+1][0]):\n",
        "    counter += 1 \n",
        "  if(arrayDE[i][0] != arrayDE[i+1][0]):\n",
        "     duration = arrayDE[i][1] - arrayDE[i-counter][1]\n",
        "     counter = 0\n",
        "     duration_vec.append(duration)"
      ],
      "metadata": {
        "id": "4cmePirnkj2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We create a new dataframe that includes the event ids, each one's duration and engagement"
      ],
      "metadata": {
        "id": "X6LgQZC9lSBH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pivot = df.loc[:, ['event_id']]\n",
        "pivotSorted = pivot.sort_values(['event_id'], ascending = [True])\n",
        "eventids = pivotSorted['event_id'].unique()\n",
        "i = 0\n",
        "k = len(duration_vec)\n",
        "k \n",
        "event = []\n",
        "for i in range(k):\n",
        "  if(i == 707 & i == 628):\n",
        "    break\n",
        "  event.append(i)\n",
        "event_dur = pd.DataFrame({'duration': duration_vec, 'event_id': eventids})\n",
        "events_engagement = df.groupby('event_id', as_index=False)['engagement'].mean()\n",
        "event_duration = event_dur.join(events_engagement['engagement'])\n",
        "event_duration.rename(columns = {'engagement':'avg_engagement'}, inplace = True)\n",
        "event_duration"
      ],
      "metadata": {
        "id": "8DUpYtDblPYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear correlation between the avg_angamenet and the duration of the events with Pearson method"
      ],
      "metadata": {
        "id": "RpkESyOAmXZg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "event_duration['avg_engagement'].corr(event_duration['duration'], method = 'spearman')"
      ],
      "metadata": {
        "id": "jcIW6uzLmMxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Non-Linear correlation between the avg_angamenet and the duration of the events with Spearman method"
      ],
      "metadata": {
        "id": "LhGnXquHm8A3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "event_duration['avg_engagement'].corr(event_duration['duration'], method = 'pearson')"
      ],
      "metadata": {
        "id": "bOzGy43bmQ9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**iv) Countries**"
      ],
      "metadata": {
        "id": "scO9mxGqaWKp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We group the mean engagement by country"
      ],
      "metadata": {
        "id": "7FY8hj55owK4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import f_oneway\n",
        "bycountryGroupList = df.groupby('country_id')['engagement'].apply(list)\n",
        "AnovaResults = f_oneway(*bycountryGroupList)\n",
        "print(f'{AnovaResults[1]:.100f}')\n",
        "AnovaResults[0].item()"
      ],
      "metadata": {
        "id": "EfFmO2Z1LP3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**v) Viewer retention**"
      ],
      "metadata": {
        "id": "KzjEikYND8Gi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We isolate the event_id, timestamp, viewer_id and engagement columns in a new dataframe (tempETV). In addition, we sort the new dataframe first by event_id then by viewer_id and finally by timestamp. Lastly we turn the sorted dataframe (tempETVESorted) into a numpy array."
      ],
      "metadata": {
        "id": "7hpHNX2spaF0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tempETVE = df.loc[:,['event_id','timestamp','viewer_id', 'engagement']]\n",
        "tempETVESorted = tempETVE.sort_values(['event_id', 'viewer_id','timestamp'], ascending=[True, True, True])\n",
        "tempETVESorted\n",
        "arrayETVE = tempETVESorted.to_numpy()"
      ],
      "metadata": {
        "id": "6BnUfk7X_tH4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We calculate every viewer's retention for each event and then place these values into a vector (retention_vector). Also we create another vector that includes viewer's average engagement for each event."
      ],
      "metadata": {
        "id": "633RIsomqjZb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "i=0\n",
        "rows = tempETVESorted.shape[0]\n",
        "retention = 0\n",
        "engSum = 0\n",
        "engMean = 0\n",
        "engCount = 0\n",
        "retention_vector = []\n",
        "engagement_vec = []\n",
        "viewer_vec = []\n",
        "ev_vec = []\n",
        "for i in range(rows):\n",
        "  if (i == rows - 1):\n",
        "    engSum += arrayETVE[i][3]\n",
        "    engCount += 1\n",
        "    engMean = engSum/engCount\n",
        "    diff = arrayETVE[i][1] - arrayETVE[i-1][1]\n",
        "  \n",
        "    if(((diff) >= 29000) & ((diff) <= 31000)):\n",
        "         retention += diff\n",
        "    retention_vector.append(retention)\n",
        "    viewer_vec.append(arrayETVE[i][2])\n",
        "    ev_vec.append(arrayETVE[i][0])\n",
        "    engagement_vec.append(engMean)\n",
        "    break\n",
        "\n",
        "  if(arrayETVE[i][0] == arrayETVE[i+1][0]): \n",
        "      if(arrayETVE[i][2] == arrayETVE[i+1][2]): \n",
        "        diff = arrayETVE[i][1] - arrayETVE[i-1][1]\n",
        "        engSum += arrayETVE[i][3]\n",
        "        engCount += 1\n",
        "\n",
        "        if(((diff) >= 29000) & ((diff) <= 31000)):\n",
        "          retention += diff\n",
        "      \n",
        "      else:\n",
        "        engSum += arrayETVE[i][3]\n",
        "        engCount += 1\n",
        "        engMean = engSum/engCount\n",
        "        diff = arrayETVE[i][1] - arrayETVE[i-1][1]\n",
        "      \n",
        "        if(((diff) >= 29000) & ((diff) <= 31000)):\n",
        "            retention += diff\n",
        "        retention_vector.append(retention)\n",
        "        viewer_vec.append(arrayETVE[i][2])\n",
        "        ev_vec.append(arrayETVE[i][0])\n",
        "        engagement_vec.append(engMean)\n",
        "        retention = 0 \n",
        "        diff = 0   \n",
        "        engMean = 0\n",
        "        engSum = 0\n",
        "        engCount = 0\n",
        "  \n",
        "  else:\n",
        "     engSum += arrayETVE[i][3]\n",
        "     engCount += 1\n",
        "     engMean = engSum/engCount\n",
        "     diff = arrayETVE[i][1] - arrayETVE[i-1][1]\n",
        "\n",
        "     if(((diff) >= 29000) & ((diff) <= 31000)):\n",
        "      retention += diff\n",
        "     retention_vector.append(retention)\n",
        "     viewer_vec.append(arrayETVE[i][2])\n",
        "     ev_vec.append(arrayETVE[i][0])\n",
        "     engagement_vec.append(engMean)\n",
        "     retention = 0 \n",
        "     diff = 0   \n",
        "     engMean = 0\n",
        "     engSum = 0\n",
        "     engCount = 0"
      ],
      "metadata": {
        "id": "EHicFswzSWyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The previous vectors are used as columns of the following new dataset (viewers_retention)"
      ],
      "metadata": {
        "id": "-CHfhnc6rtaF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "viewers_retention = pd.DataFrame({'retention': retention_vector, 'engagement': engagement_vec, 'event_id': ev_vec, 'viewer_id': viewer_vec})\n",
        "viewers_retention"
      ],
      "metadata": {
        "id": "Ap1vEWI-cpZu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Non-Linear correlation between the average viewer's engagement for each event and their corresponding retention with Spearman methon"
      ],
      "metadata": {
        "id": "y_BtrsIEsoiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "viewers_retention['engagement'].corr(viewers_retention['retention'], method = 'spearman')"
      ],
      "metadata": {
        "id": "AI4wRwQDdfol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear correlation between the average viewer's engagement for each event and their corresponding retention with Pearson methon"
      ],
      "metadata": {
        "id": "_e9n8wfosEs-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "viewers_retention['engagement'].corr(viewers_retention['retention'], method = 'pearson')"
      ],
      "metadata": {
        "id": "UlCvWzj5dzMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# feature engineering"
      ],
      "metadata": {
        "id": "HWC6kNWEGlbB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "NUMBER OF VIEWERS IN AN EVENT INTO DF"
      ],
      "metadata": {
        "id": "b2aGwIva0mJv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "event_vector = df[['event_id']].to_numpy().astype(int)\n",
        "a = len(event_vector)"
      ],
      "metadata": {
        "id": "br0lFBhvRcCB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "event_628_NV = {'event_id': 628, 'avg_engagement' : 0, 'number_of_viewers':0 }\n",
        "groupedbyEvent = groupedbyEvent.append(event_628_NV, ignore_index = True)\n",
        "event_702_NV = {'event_id': 702, 'avg_engagement' : 0, 'number_of_viewers':0 }\n",
        "groupedbyEvent = groupedbyEvent.append(event_702_NV, ignore_index = True)\n",
        "event_707_NV = {'event_id': 707, 'avg_engagement' : 0, 'number_of_viewers':0 }\n",
        "groupedbyEvent = groupedbyEvent.append(event_707_NV, ignore_index = True)"
      ],
      "metadata": {
        "id": "HB5xgeNB2VzK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "groupedbyEvent = groupedbyEvent.groupby(['event_id'], as_index=False)['number_of_viewers'].sum()\n",
        "\n",
        "numofviewers_array = groupedbyEvent[['event_id','number_of_viewers']].to_numpy().astype(int)"
      ],
      "metadata": {
        "id": "br8prdQ4dNj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "event_vector = df[['event_id']].to_numpy().astype(int)\n",
        "entries = event_vector.shape[0]\n",
        "\n",
        "i=0\n",
        "pos=0\n",
        "numViewers_list = []\n",
        "for i in range(entries):\n",
        "  pos = event_vector[i].item()\n",
        "  numViewers_list.append(numofviewers_array[pos][1])"
      ],
      "metadata": {
        "id": "RFIRAOfnSdGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['viewers_event'] = numViewers_list\n",
        "df"
      ],
      "metadata": {
        "id": "Je0N8U9281zD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "EVENT DURATION INTO DF"
      ],
      "metadata": {
        "id": "M_7q_k9U-V77"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "event_628_dur = {'duration' : 0, 'event_id': 628, 'avg_engagement':0 }\n",
        "event_duration = event_duration.append(event_628_dur, ignore_index = True)\n",
        "event_702_dur = {'duration' : 0, 'event_id': 702, 'avg_engagement':0 }\n",
        "event_duration = event_duration.append(event_702_dur, ignore_index = True)\n",
        "event_707_dur = {'duration' : 0, 'event_id': 707, 'avg_engagement':0 }\n",
        "event_duration = event_duration.append(event_707_dur, ignore_index = True)"
      ],
      "metadata": {
        "id": "rbbPGZJe-VNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "event_duration = event_duration.groupby(['event_id'], as_index=False)['duration'].sum()\n",
        "\n",
        "eventDur_array = event_duration[['event_id','duration']].to_numpy().astype(int)"
      ],
      "metadata": {
        "id": "tf3ndEAn-__2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "event_vector = df[['event_id']].to_numpy().astype(int)\n",
        "entries = event_vector.shape[0]\n",
        "\n",
        "i=0\n",
        "pos=0\n",
        "eventDur_list = []\n",
        "for i in range(entries):\n",
        "  pos = event_vector[i].item()\n",
        "  eventDur_list.append(eventDur_array[pos][1])"
      ],
      "metadata": {
        "id": "D1EomOct_YIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['event_duration'] = eventDur_list\n",
        "df"
      ],
      "metadata": {
        "id": "jn3yPZSF_qsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "VIEWER RETENTION INTO DF"
      ],
      "metadata": {
        "id": "POMxdHcPAGAq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retention_array = viewers_retention.to_numpy()\n",
        "\n",
        "temp = df[['viewer_id','event_id']].to_numpy().astype(int)\n",
        "events = event_vector.shape[0]\n",
        "len_retention = viewers_retention.shape[0]\n",
        "\n",
        "i = 0\n",
        "\n",
        "event_pos = 0\n",
        "viewer_pos = 0\n",
        "\n",
        "\n",
        "min_index = 0\n",
        "max_index = len(retention_array) -1\n",
        "\n",
        "retention_list = []\n",
        "\n",
        "for i in range(events):\n",
        "  j=0\n",
        "  min_index = 0\n",
        "  max_index = len(retention_array) -1\n",
        "  viewer_pos = temp[i][0].item()\n",
        "  event_pos = temp[i][1].item()\n",
        "\n",
        "  while ((max_index >= min_index) & (j==0)):\n",
        "        mid_index =(max_index+min_index)//2\n",
        "\n",
        "        if(event_pos==retention_array[mid_index][2]):\n",
        "            if(viewer_pos==retention_array[mid_index][3]):\n",
        "              retention_list.append(retention_array[mid_index][0])\n",
        "              j=1\n",
        "            elif (retention_array[mid_index][3] < viewer_pos):\n",
        "                min_index = mid_index+1\n",
        "            else:\n",
        "                max_index = mid_index-1\n",
        "        elif (retention_array[mid_index][2] < event_pos):\n",
        "            min_index = mid_index+1\n",
        "        else :\n",
        "              max_index = mid_index-1"
      ],
      "metadata": {
        "id": "KZAr7doQALJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(retention_list)"
      ],
      "metadata": {
        "id": "5REsRbN11AzU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['viewer_retention'] = retention_list\n",
        "df"
      ],
      "metadata": {
        "id": "U1gsWygVFNrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/NEW_dataset.csv\")\n",
        "df['datetime'] = pd.to_datetime(df['timestamp'], unit='ms')\n",
        "df['dayWeek'] = df['datetime'].dt.day_name()\n",
        "df"
      ],
      "metadata": {
        "id": "xHl8hdnHGa1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_features = df[['event_id', 'viewer_id', 'country_id', 'viewer_type', 'viewers_event', 'event_duration', 'viewer_retention', 'dayWeek', 'qoe', 'engagement']]\n",
        "data_features"
      ],
      "metadata": {
        "id": "T0p4SfMVCIul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encode the categorical values"
      ],
      "metadata": {
        "id": "mmXPwaPNEsOb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "viewer_type_le = preprocessing.LabelEncoder()\n",
        "viewer_type_one_hot = preprocessing.OneHotEncoder()\n",
        "\n",
        "data_features['viewer_type_one_hot'] = viewer_type_one_hot.fit_transform(data_features[\"viewer_type\"].values.reshape(-1,1)).toarray().tolist()"
      ],
      "metadata": {
        "id": "bE3ArDtREtme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_features.drop(columns=[\"viewer_type\"])"
      ],
      "metadata": {
        "id": "Y_-nCvFNWfdd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dayWeek_le = preprocessing.LabelEncoder()\n",
        "dayWeek_one_hot = preprocessing.OneHotEncoder()\n",
        "\n",
        "data_features['dayWeek_one_hot'] = dayWeek_one_hot.fit_transform(data_features[\"dayWeek\"].values.reshape(-1,1)).toarray().tolist()"
      ],
      "metadata": {
        "id": "jTBz7ljcGjMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "viewers_event_le = preprocessing.MinMaxScaler()\n",
        "data_features[\"viewers_event_scaled\"] = viewers_event_le.fit_transform(data_features[\"viewers_event\"].values.reshape(-1,1))"
      ],
      "metadata": {
        "id": "qRkbYJ2nG23g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "event_duration_le = preprocessing.MinMaxScaler()\n",
        "data_features[\"event_duration_scaled\"] = event_duration_le.fit_transform(data_features[\"event_duration\"].values.reshape(-1,1))"
      ],
      "metadata": {
        "id": "qlMvW5C9aYLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "viewer_retention_le = preprocessing.MinMaxScaler()\n",
        "data_features[\"viewer_retention_scaled\"] = viewer_retention_le.fit_transform(data_features[\"viewer_retention\"].values.reshape(-1,1))"
      ],
      "metadata": {
        "id": "fqk2IYdAaYiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create \"Network Anomaly\" column to check if an entry occures network anomal"
      ],
      "metadata": {
        "id": "D2JABL1w1IIw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = data_features.drop(columns = ['viewer_type', 'event_duration', 'viewers_event', 'viewer_retention', 'dayWeek'])\n",
        "df"
      ],
      "metadata": {
        "id": "vIo0tkcLTJ55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ax = sns.barplot(y=\"qoe\", data=df, ci='sd', capsize=0.3, errwidth=2).set(title='Qoe distribution barplot')"
      ],
      "metadata": {
        "id": "6umxnsh71Ey1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from statistics import stdev\n",
        "\n",
        "qoe_vector = df[\"qoe\"].values.tolist() #convert qoe column to vector\n",
        "qoe_stdev = stdev(qoe_vector) #compute the stdev of qoe;\n",
        "i=0\n",
        "k=df[\"qoe\"].shape[0]\n",
        "network_anomaly = [0]*k # 0 for no, 1 for yes\n",
        "\n",
        "for i in range(k):\n",
        "  if (qoe_vector[i]<1-qoe_stdev):\n",
        "    network_anomaly[i] = 1"
      ],
      "metadata": {
        "id": "2ltAGoIp1HtT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"network_anomaly\"]=network_anomaly\n",
        "df"
      ],
      "metadata": {
        "id": "O7A7hA7I1WVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train/Val/Test Split"
      ],
      "metadata": {
        "id": "mfYoP4Ppk2uP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_val_test_split(dataset, sequence_length):\n",
        "\n",
        "    dataset, n_events, n_qoe, n_country, n_viewer, event_interactions = preprocess_dataset(dataset, sequence_length)\n",
        "\n",
        "    dataset_grouped = dataset.groupby(['event_id', 'viewer_id', 'country_id', 'viewer_type_one_hot', 'dayWeek_one_hot', 'viewers_event_scaled', 'event_duration_scaled', 'viewer_retention_scaled']).cumcount(ascending=False)\n",
        "\n",
        "    train = dataset[dataset_grouped > 1]\n",
        "    val = dataset[dataset_grouped == 1]\n",
        "    test = dataset[dataset_grouped == 0]\n",
        "    return train, val, test, n_events, n_qoe, n_country, n_viewer, event_interactions\n",
        "\n",
        "\n",
        "def preprocess_dataset(dataset, sequence_length):\n",
        "    n_events = dataset['event_id'].nunique()\n",
        "    n_qoe = dataset['network_anomaly'].nunique()\n",
        "    n_country = dataset['country_id'].nunique() \n",
        "    n_viewer = dataset['viewer_id'].nunique() \n",
        "    \n",
        "    def generate_sequences(x):\n",
        "        qoe_list = x.values\n",
        "        result = []\n",
        "        for i in range(len(qoe_list) - 1):\n",
        "            sequence = np.zeros(sequence_length, dtype=int) + n_qoe\n",
        "\n",
        "            start_index = max(0, i - sequence_length)\n",
        "            end_index = i + 1\n",
        "            for idx, qoe in enumerate(reversed(qoe_list[start_index:end_index])):\n",
        "                sequence[sequence_length - 1 - idx] = int(qoe)\n",
        "            result.append((sequence, qoe_list[i+1]))\n",
        "        return result\n",
        "\n",
        "    event_interactions = dataset.groupby(['event_id', 'viewer_id', 'country_id', 'viewer_type_one_hot', 'dayWeek_one_hot', 'viewers_event_scaled', 'event_duration_scaled', 'viewer_retention_scaled'])['network_anomaly'].apply(list).to_dict()\n",
        "    \n",
        "    \n",
        "    dataset = dataset.groupby(['event_id', 'viewer_id', 'country_id', 'viewer_type_one_hot', 'dayWeek_one_hot', 'viewers_event_scaled', 'event_duration_scaled', 'viewer_retention_scaled'])['network_anomaly'].apply(generate_sequences).reset_index()\n",
        "\n",
        "    dataset = dataset.explode('network_anomaly')\n",
        "    dataset[['features','label']] =  pd.DataFrame(dataset['network_anomaly'].tolist(), index=dataset.index)\n",
        "    \n",
        "    return dataset[['event_id', 'viewer_id', 'country_id', 'viewer_type_one_hot',  'dayWeek_one_hot', 'viewers_event_scaled', 'event_duration_scaled', 'viewer_retention_scaled', 'features','label']], n_events, n_qoe, n_country, n_viewer, event_interactions\n",
        "    "
      ],
      "metadata": {
        "id": "zU_x125Pxmcp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train, val, test, n_events, n_qoe, n_country, n_viewer,train_u_i_dict = train_val_test_split(df, 5)"
      ],
      "metadata": {
        "id": "pUNnOI8LxIPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom Data Loader"
      ],
      "metadata": {
        "id": "fUN9Y0avJ48e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train"
      ],
      "metadata": {
        "id": "MtRc-GKXz6YG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train[train['label'].isnull()]) # check if there are null values"
      ],
      "metadata": {
        "id": "HmaNNtSrppFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SequentialDataset(Dataset):\n",
        "    def __init__(self, dataset, train):\n",
        "        self.dataset = dataset\n",
        "        if not train:\n",
        "            self.dataset.sort_values(by='event_id',inplace=True)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.dataset.shape[0]\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        row = self.dataset.iloc[index]\n",
        "        event = int(row['event_id'])\n",
        "        viewer = int(row['viewer_id'])\n",
        "        country = int(row['country_id'])\n",
        "        viewer_type = row['viewer_type_one_hot']\n",
        "        dayWeek = row['dayWeek_one_hot']\n",
        "        viewers_event = row['viewers_event_scaled']\n",
        "        event_duration = row['event_duration_scaled']\n",
        "        viewer_retention = row['viewer_retention_scaled']\n",
        "        sequence = row['features']\n",
        "        target =  int(row['label'])\n",
        "\n",
        "        return {'events':np.tile(event, 5), 'viewers':np.tile(viewer, 5), 'countries':np.tile(country, 5), 'viewer_types':viewer_type, 'dayWeeks':dayWeek, 'viewers_event':np.tile(viewers_event, 5), 'event_duration':np.tile(event_duration, 5), 'viewer_retention':np.tile(viewer_retention, 5), 'seqs': sequence, 'targets':target}"
      ],
      "metadata": {
        "id": "NRIle7OnKtwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainLoader = DataLoader(SequentialDataset(train, True), batch_size=512, shuffle=True)\n",
        "valLoader = DataLoader(SequentialDataset(val, False), batch_size=64)\n",
        "testLoader = DataLoader(SequentialDataset(test, False), batch_size=64)"
      ],
      "metadata": {
        "id": "dE9VOhBVPDKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, batch in enumerate(trainLoader): #for batch_size = 2\n",
        "  print (batch)\n",
        "  break"
      ],
      "metadata": {
        "id": "Mub2Mq9TVmZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "class LSTM(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, n_events, n_viewer, n_country, n_qoe, embed_dim, hidden_size,  u_i_dict, lr = 1e-3): \n",
        "        super(LSTM, self).__init__()\n",
        "\n",
        "        self.events_embedding = nn.Embedding(n_events, embed_dim)\n",
        "        self.viewers_embedding = nn.Embedding(n_viewer, embed_dim)\n",
        "        self.countries_embedding = nn.Embedding(n_country, embed_dim)\n",
        "        self.qoes_embedding = nn.Embedding(n_qoe + 1, embed_dim, padding_idx=n_qoe)\n",
        "\n",
        "        self.lstm = nn.LSTM(input_size= 4*embed_dim + 3 ,  \n",
        "                            hidden_size=hidden_size,\n",
        "                            num_layers=1,\n",
        "                            bias = True,\n",
        "                            batch_first=True,\n",
        "                            dropout=0.,\n",
        "                            bidirectional=False,\n",
        "                            proj_size=0)\n",
        "        \n",
        "        self.l1 = nn.Linear(in_features=hidden_size, out_features=hidden_size*2)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.l2 = nn.Linear(in_features= hidden_size*2, out_features= 1)\n",
        "        self.sigmoid=nn.Sigmoid()\n",
        "        self.loss = nn.BCELoss()\n",
        "        self.lr = lr\n",
        "    \n",
        "    def predict(self, batch):\n",
        "        events= self.events_embedding(batch['events'])\n",
        "        viewers = self.viewers_embedding(batch['viewers'])\n",
        "        countries = self.countries_embedding(batch['countries'])\n",
        "        viewers_event = (batch['viewers_event'].long()).unsqueeze(-1)\n",
        "        event_duration = (batch['event_duration'].long()).unsqueeze(-1)\n",
        "        viewer_retention = (batch['viewer_retention'].long()).unsqueeze(-1)\n",
        "        sequens = self.qoes_embedding(batch['seqs'])\n",
        "\n",
        "        input = torch.cat((events, viewers, countries, sequens, viewers_event, event_duration, viewer_retention), dim=2)\n",
        "\n",
        "        output, _ = self.lstm(input)\n",
        "        output = output[:,-1,:]\n",
        "        output = self.l1(output)\n",
        "        output = self.dropout(output)\n",
        "        output = self.l2(output)\n",
        "        output = self.sigmoid(output)\n",
        "        return output\n",
        "        \n",
        "    def training_step(self, batch, batch_idx):\n",
        "        y = batch['targets']\n",
        "        y = y.unsqueeze(1)\n",
        "\n",
        "        y_hat = self.predict(batch)\n",
        "        \n",
        "        loss = self.loss(y_hat,y.float())\n",
        "        self.log('train_loss', loss)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        y = batch['targets']\n",
        "        y = y.unsqueeze(1)\n",
        "\n",
        "        y_hat = self.predict(batch)\n",
        "        \n",
        "        loss = self.loss(y_hat,y.float())\n",
        "        self.log('val_loss', loss)\n",
        "        return loss\n",
        "    \n",
        "    def test_step(self, batch, batch_idx):\n",
        "        x= self.user_embedding(batch['events'])\n",
        "        y = batch['targets']\n",
        "        y = y.unsqueeze(1)\n",
        "\n",
        "        y_hat = self.predict(batch)\n",
        "        loss = self.loss(y_hat, y.float())\n",
        "\n",
        "        #evaluation\n",
        "        eval_score = f1_score(y,y_hat)\n",
        "        print(eval_score)\n",
        "        \n",
        "        return loss\n",
        "        \n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
        "        return optimizer\n",
        "\n",
        "embed_dim = 256\n",
        "hidden_dim= 512\n",
        "model = LSTM(n_events, n_viewer, n_country, n_qoe, embed_dim, hidden_dim, train_u_i_dict) \n",
        "\n",
        "trainer = pl.Trainer(gpus=1,\n",
        "                     max_epochs=2,\n",
        "                     progress_bar_refresh_rate=20, # set progress_bar_refresh_rate to 0, so that it doesnt crash colab\n",
        "                     check_val_every_n_epoch=1 \n",
        "                     )\n",
        "\n",
        "trainer.fit(model, train_dataloaders=trainLoader, val_dataloaders=valLoader)\n",
        "test = trainer.test(dataloaders = testLoader, verbose=True)"
      ],
      "metadata": {
        "id": "uyP2z7ynZPH-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "KdeMvMOdIzTN",
        "Chpp2Mg5_AIa",
        "T4GNQTDxJVqi",
        "hg2v7IhKAA5G",
        "VrYUfMhZTAH0",
        "P1d_DamfE_jz",
        "mfth4LQHcI07",
        "F2_ritjYW_YS",
        "HWC6kNWEGlbB",
        "mmXPwaPNEsOb",
        "mfYoP4Ppk2uP",
        "fUN9Y0avJ48e"
      ],
      "name": "Data_Analysis_2696_2898_2760.ipynb",
      "provenance": [],
      "private_outputs": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}